{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/heizsen/Ai/blob/main/NLP_Demo_Basic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tutorial adopted from: https://medium.com/pythoneers/basics-of-natural-language-processing-in-10-minutes-2ed51e6d5d32"
      ],
      "metadata": {
        "id": "0iac_Ca-SqWA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"popular\")\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmjq4czER4Le",
        "outputId": "ce48d6ec-61e7-46d9-a5f0-5dcbee3c9509"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization** is the process of dividing the whole text into tokens."
      ],
      "metadata": {
        "id": "HCcbhqIFUnjt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QceZVINmQ90m",
        "outputId": "141a2844-e98f-4b3b-c03c-758da25b6c43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello there, how are you doing today?', 'The weather is great today.', 'The sky is blue.', 'Python is awesome']\n",
            "['Hello', 'there', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'weather', 'is', 'great', 'today', '.', 'The', 'sky', 'is', 'blue', '.', 'Python', 'is', 'awesome']\n"
          ]
        }
      ],
      "source": [
        "example_text = \"Hello there, how are you doing today? The weather is great today. The sky is blue. Python is awesome\"\n",
        "\n",
        "# sent_tokenize (Separated by sentence)\n",
        "sentences = sent_tokenize(example_text)\n",
        "print(sentences)\n",
        "##word_tokenize (Separated by words)\n",
        "words = word_tokenize(example_text)\n",
        "print(words)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In general, **stopwords** are the words in any language which does not add much meaning to a sentence. In NLP stopwords are those words which are not important in analyzing the data."
      ],
      "metadata": {
        "id": "2AJmnxP3UY6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stopwords\n",
        "from nltk.corpus import stopwords\n",
        "print(stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EVT-CfNSawB",
        "outputId": "5a774a98-01dd-4d38-ecfb-adf93a1cd4e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "text = 'he is a good boy. he is very good in coding'\n",
        "words = word_tokenize(text)\n",
        "words_without_stopwords = [word for word in words if word not in stopwords.words('english')]\n",
        "print(words_without_stopwords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePWSBa3CTKj0",
        "outputId": "29d3390a-bfa1-4071-ab1b-d356174871ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['good', 'boy', '.', 'good', 'coding']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stemming** is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma."
      ],
      "metadata": {
        "id": "iuE8knjFUNNV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()    ## Creating an object for porterstemmer\n",
        "example_words = ['earn',\"earning\",\"earned\",\"earns\"]  ##Example words\n",
        "for w in example_words:\n",
        "    print(ps.stem(w))    ##Using ps object stemming the word"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fY0O1EvRT6Dm",
        "outputId": "237a9f6b-b8f8-47c1-bfdd-b3aba758a54f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "earn\n",
            "earn\n",
            "earn\n",
            "earn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lemmatization** does the same work as stemming, the difference is that lemmatization returns a meaningful word.\n",
        "(Commonly used in Chatbot)\n"
      ],
      "metadata": {
        "id": "HIKqhioxVCd6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer() ## Create object for lemmatizer\n",
        "example_words = ['history','formality','changes']\n",
        "for w in example_words:\n",
        "    print(lemmatizer.lemmatize(w))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_l-DmUEJVMLf",
        "outputId": "b31a1230-16aa-46bb-800e-a348376eda64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "history\n",
            "formality\n",
            "change\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**WordNet** is the lexical database i.e. dictionary for the English language, specifically designed for natural language processing.\n",
        "We can use wordnet for finding synonyms and antonyms."
      ],
      "metadata": {
        "id": "Wt6J6UPbVpIp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet\n",
        "synonyms = []   ## Creaing an empty list for all the synonyms\n",
        "antonyms =[]    ## Creaing an empty list for all the antonyms\n",
        "for syn in wordnet.synsets(\"happy\"): ## Giving word\n",
        "    for i in syn.lemmas():        ## Finding the lemma,matching\n",
        "        synonyms.append(i.name())  ## appending all the synonyms\n",
        "        if i.antonyms():\n",
        "            antonyms.append(i.antonyms()[0].name()) ## antonyms\n",
        "print(set(synonyms)) ## Converting them into set for unique values\n",
        "print(set(antonyms))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UeDnWdWVot7",
        "outputId": "d1f88e4f-73d0-469c-f7f6-ead1a65e4c09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'happy', 'well-chosen', 'felicitous', 'glad'}\n",
            "{'unhappy'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part of Speech (PoS) Tagging** is a process of converting a sentence to forms — a list of words, a list of tuples (where each tuple is having a form (word, tag)). The tag in the case is a part-of-speech tag and signifies whether the word is a noun, adjective, verb, and so on."
      ],
      "metadata": {
        "id": "CxBDKR2KVzVn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        " CC coordinating conjunction\n",
        " CD cardinal digit\n",
        " DT determiner\n",
        " EX existential there (like: “there is” … think of it like “there”)\n",
        " FW foreign word\n",
        " IN preposition/subordinating conjunction\n",
        " JJ adjective ‘big’\n",
        " JJR adjective, comparative ‘bigger’\n",
        " JJS adjective, superlative ‘biggest’\n",
        " LS list marker 1)\n",
        " MD modal could, will\n",
        " NN noun, singular ‘desk’\n",
        " NNS noun plural ‘desks’\n",
        " NNP proper noun, singular ‘Harrison’\n",
        " NNPS proper noun, plural ‘Americans’\n",
        " PDT predeterminer ‘all the kids’\n",
        " POS possessive ending parent’s\n",
        " PRP personal pronoun I, he, she\n",
        " PRP possessive pronoun my, his, hers\n",
        " RB adverb very, silently,\n",
        " RBR adverb, comparative better\n",
        " RBS adverb, superlative best\n",
        " RP particle give up\n",
        " TO to go ‘to’ the store.\n",
        " UH interjection errrrrrrrm\n",
        " VB verb, base form take\n",
        " VBD verb, past tense took\n",
        " VBG verb, gerund/present participle taking\n",
        " VBN verb, past participle taken\n",
        " VBP verb, sing. present, non-3d take\n",
        " VBZ verb, 3rd person sing. present takes\n",
        " WDT wh-determiner which\n",
        " WP wh-pronoun who, what\n",
        " WP possessive wh-pronoun whose\n",
        " WRB wh-abverb where, when\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "FJpioperW2YI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = '''\n",
        "An sincerity so extremity he additions. Her yet there truth merit. Mrs all projecting favourable now unpleasing. Son law garden chatty temper. Oh children provided to mr elegance marriage strongly. Off can admiration prosperous now devonshire diminution law.\n",
        "'''\n",
        "words = word_tokenize(sample_text)\n",
        "print(nltk.pos_tag(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwTjn74kWdLE",
        "outputId": "cd0c7c72-724e-4e73-e74f-bbbf3e15b783"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('An', 'DT'), ('sincerity', 'NN'), ('so', 'RB'), ('extremity', 'NN'), ('he', 'PRP'), ('additions', 'VBZ'), ('.', '.'), ('Her', 'PRP$'), ('yet', 'RB'), ('there', 'EX'), ('truth', 'NN'), ('merit', 'NN'), ('.', '.'), ('Mrs', 'NNP'), ('all', 'DT'), ('projecting', 'VBG'), ('favourable', 'JJ'), ('now', 'RB'), ('unpleasing', 'VBG'), ('.', '.'), ('Son', 'NNP'), ('law', 'NN'), ('garden', 'NN'), ('chatty', 'JJ'), ('temper', 'NN'), ('.', '.'), ('Oh', 'UH'), ('children', 'NNS'), ('provided', 'VBD'), ('to', 'TO'), ('mr', 'VB'), ('elegance', 'NN'), ('marriage', 'NN'), ('strongly', 'RB'), ('.', '.'), ('Off', 'CC'), ('can', 'MD'), ('admiration', 'VB'), ('prosperous', 'JJ'), ('now', 'RB'), ('devonshire', 'VBP'), ('diminution', 'NN'), ('law', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Bag of words**"
      ],
      "metadata": {
        "id": "xgHkWCNNXJr2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After cleaning text, we need to convert the text into some kind of numerical representation called vectors (bag-of-words) so that we can feed the data to a machine learning model for further processing."
      ],
      "metadata": {
        "id": "HJDh2ppRXV3A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "sent1 = he is a good boy\n",
        "sent2 = she is a good girl\n",
        "sent3 = boy and girl are good\n",
        "        |\n",
        "        |\n",
        "  After removal of stopwords , lematization or stemming\n",
        "sent1 = good boy\n",
        "sent2 = good girl\n",
        "sent3 = boy girl good  \n",
        "        | ### Now we will calculate the frequency for each word by\n",
        "        |     calculating the occurrence of each word\n",
        "word  frequency\n",
        "good     3\n",
        "boy      2\n",
        "girl     2\n",
        "         | ## Then according to their occurrence we assign o or 1\n",
        "         |    according to their occurrence in the sentence\n",
        "         | ## 1 for present and 0 fot not present\n",
        "         f1  f2   f3\n",
        "        girl good boy   \n",
        "sent1    0    1    1     \n",
        "sent2    1    0    1\n",
        "sent3    1    1    1\n",
        "### After this we pass the vector form to machine learning model\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "GcQokX0CYPpf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = ['he is a good boy', 'she is a good girl', 'boy and girl are good']\n",
        "corpus = []\n",
        "for sent in sentences:\n",
        "    words  = word_tokenize(sent)\n",
        "    texts = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
        "    text = ' '.join(texts)\n",
        "    corpus.append(text)\n",
        "print(corpus)   #### Cleaned Data\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer() ## Creating Object for CountVectorizer\n",
        "word_counts = cv.fit_transform(corpus).toarray()\n",
        "print(word_counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lotdeuRAXrmo",
        "outputId": "bd3a3f89-905b-4716-fc6c-fdd4510037c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['good boy', 'good girl', 'boy girl good']\n",
            "[[1 0 1]\n",
            " [0 1 1]\n",
            " [1 1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification of Text/Article using Bag-of-Words\n",
        "\n",
        "Given 3 articles with their respective topic label, we will classify the topic of a new article."
      ],
      "metadata": {
        "id": "7p2FplD_3hKO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Three texts with labeled topic\n",
        "text_sport = \"Liverpool held off a late charge from Tottenham after Mohamed Salah struck twice to win 2-1 in north London and lift themselves back into the Premier League top-four race. Jurgen Klopp side had suffered shock defeats to relegation-threatened pair Nottingham Forest and Leeds in their last two league outings but started fast against Spurs, with a sharp touch and finish into the bottom corner from in-form Salah giving them the lead on 11 minutes. Ivan Perisic had a header deflected onto the post by Liverpool goalkeeper Alisson and Ryan Sessegnon saw a penalty shout from a challenge by Trent Alexander-Arnold waved away as Spurs came to life - but the Reds struck again just before the break thanks to a gift from Eric Dier. The centre-back miscued a header towards his own goal and Salah (40) raced through to chip in his ninth goal in eight games. Tottenham were sent out early for the second half and - not for the first time this season - were better after the break, with Alisson again pushing a Perisic effort onto the woodwork before Harry Kane (70) fired home a brilliant strike when played in by sub Dejan Kulusevski. Rodrigo Bentancur went close with a couple of headers and Kane glanced wide as Spurs desperately sought another late goal but Liverpool clung on for their first away win in the Premier League this season to move up to eighth and within seven points of fourth-placed Spurs, with a game in hand.\"\n",
        "text_medical = \"There is growing popularity of the Hospital Incident Command System (HICS) as an organizational tool for hospital management in the COVID-19 pandemic. We specifically describe implementation of HICS at the Isfahan province reference hospital (Isabn-e-Maryam) during the COVID-19 pandemic and try to explore performance of it. Methods: To document the actions taken during the COVID-19 pandemic, standard, open-ended interviews were conducted with individuals occupying activated HICS leadership positions during the event. A checklist based on the job action sheets of the HICS was used for performance assessment. Results: With the onset of the pandemic, hospital director revised ICS structure that adheres to span of better control of COVID-19. Methods of expanding hospital inpatient capacity to enable surge capacity were considered. The highest performance score was in the field of planning. Performance was intermediate in Financial/Administration section and good in other fields. Discussion: In the current COVID-19 pandemic, establishing HICS with some consideration about long-standing events can help improve communication, resource use, staff and patient protection, and maintenance of roles.\"\n",
        "text_finance = \"According to Fidelity’s Financial Resolutions survey, saving more money was the number one-resolution for respondents. Close to half (43%) said this was a goal they wanted to work toward in the new year. Building a nest egg can help you pay for big ticket items — like a house, a vacation, a wedding or even just an expensive item you really want — without taking on additional debt. Having savings can also come in handy if an emergency expense comes up. If you’re looking to increase your savings in the new year, it helps to start small — even if you’re only transferring $10 a week into your savings account. Starting small helps you build a muscle for saving. This way, when you receive salary bumps, bonuses and gift money, you’ve already gotten into the habit of saving, and you’ll be more likely to transfer that money to your savings account. You may also want to consider automating that process instead of just manually moving money into your savings. Relying on manual transfers leaves a lot of room for procrastination — and before we know it, we’ve spent the money we intended to save. But when you set up automatic transfers into your savings account, you take away the need to make that decision altogether. You can usually schedule automatic transfers through your bank’s mobile app. Lastly, if you want to see your savings grow just a little faster, you can opt for a high-yield savings account instead of a traditional savings account. High-yield savings accounts — like the Marcus by Goldman Sachs Online Savings Account or the Ally Online Savings Account — pay you more in interest each month compared to traditional savings accounts.\"\n",
        "\n",
        "texts = [text_sport, text_medical, text_finance]\n",
        "bow_keys = []\n",
        "corpus_texts = []\n",
        "for text in texts:\n",
        "    words  = word_tokenize(text)\n",
        "    texts = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
        "    bow_keys += texts\n",
        "    text = ' '.join(texts)\n",
        "    corpus_texts.append(text)\n",
        "bow_keys = set(bow_keys)\n",
        "print(bow_keys)   #### Cleaned Data\n",
        "print(corpus_texts)   #### Cleaned Data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMOR1wpmejZd",
        "outputId": "91f56ba4-d8d6-42f3-b8aa-2d6a78033ca7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Financial', 'looking', 'salary', 'take', 'another', 'surge', 'role', 'manually', 'patient', 'receive', 'interview', 'enable', 'Trent', 'Kane', 'COVID-19', 'minute', 'Ivan', '43', '%', 'know', 'waved', 'span', 'sub', 'expanding', 'vacation', 'communication', 'set', 'fourth-placed', 'traditional', 'revised', 'came', 'really', 'altogether', 'side', 'centre-back', 'fast', 'Rodrigo', 'held', 'establishing', 'played', 'If', 'need', 'expensive', 'opt', 'miscued', 'maintenance', '10', '’', 'checklist', 'touch', 'may', 'want', 'Savings', 'considered', 'spent', 'second', 'Nottingham', 'taking', 'tool', 'muscle', 'hospital', 'Leeds', 'adheres', 'week', 'London', 'finish', 'compared', 'Liverpool', 'charge', 'fired', 'schedule', 'activated', 'long-standing', 'Perisic', 'pay', 'brilliant', 'couple', 'ticket', 'staff', '(', ')', 'Alisson', 'goal', 'shock', 'improve', 'save', '.', 'sharp', 'gift', 'make', 'top-four', 'conducted', 'help', 'consideration', 'implementation', 'Building', 'first', 'time', 'position', 'room', 'High-yield', 'Mohamed', 'In', 'small', 'move', 'come', 'job', 'glanced', 'lift', 'Sessegnon', 'account', 'There', '-', 'good', 'game', 'app', 'way', 'Ryan', 'back', 'province', 'Klopp', 'process', '$', 'faster', 'Starting', 'corner', 'Command', 'thanks', 'Lastly', 'even', 'performance', 'season', 'Fidelity', 'sheet', 'Reds', 'one-resolution', 'already', 'Alexander-Arnold', 'management', 'leaf', 'defeat', 'Dier', 'Close', 'automating', 'desperately', 'bottom', 'emergency', 'penalty', 'assessment', 'resource', 'raced', 'popularity', 'organizational', 'Ally', 'manual', 'use', 'number', 'seven', 'Harry', 'sent', 'We', 'said', 'survey', 'bump', 'But', 'likely', 'Resolutions', 'header', 'You', 'habit', 'also', 'effort', 'highest', 'deflected', '40', 'struck', 'The', 'lot', 'With', 'Financial/Administration', 'Dejan', 'To', 'pair', 'relegation-threatened', 'ICS', 'inpatient', 'moving', 'sought', 'Isabn-e-Maryam', 'Methods', 'wedding', 'big', ':', 'strike', 'towards', 'close', 'saving', 'egg', 'Kulusevski', 'specifically', 'Forest', 'saw', 'Tottenham', 'transferring', 'better', 'shout', 'eight', 'event', 'standard', 'director', 'taken', 'current', 'Hospital', '70', 'Spurs', 'Bentancur', 'based', 'went', 'last', ',', 'twice', 'item', 'high-yield', 'onto', 'increase', 'outing', 'try', 'document', 'woodwork', 'intended', 'score', 'two', 'suffered', 'toward', 'Goldman', 'pandemic', 'Results', 'hand', 'Having', 'see', 'Salah', 'build', 'usually', 'like', 'onset', 'transfer', 'gotten', 'occupying', 'late', 'Sachs', 'individual', 'explore', 'point', 'nest', 'goalkeeper', 'field', 'money', 'used', 'capacity', 'little', 'within', 'chip', 'break', 'This', '11', 'lead', 'control', 'additional', 'action', 'clung', 'half', 'instead', 'started', 'respondent', 'Eric', 'bonus', 'ninth', 'pushing', 'Account', 'life', 'leadership', 'A', 'consider', 'reference', 'league', 'Performance', '2-1', 'year', 'in-form', 'procrastination', 'home', 'intermediate', 'HICS', 'post', '—', 'grow', 'new', 'Relying', 'wanted', 'protection', 'Incident', 'start', 'giving', 'debt', 'describe', 'automatic', 'house', 'section', 'According', 'handy', 'month', 'without', 'away', 'System', 'Premier', 'decision', 'race', 'Jurgen', 'Marcus', 'Isfahan', 'Discussion', 'Online', 'work', 'mobile', 'planning', 'north', 'expense', 'early', 'open-ended', 'structure', 'bank', 'challenge', 'interest', 'win', 'League', 'wide', 'growing', 'eighth'}\n",
            "['Liverpool held late charge Tottenham Mohamed Salah struck twice win 2-1 north London lift back Premier League top-four race . Jurgen Klopp side suffered shock defeat relegation-threatened pair Nottingham Forest Leeds last two league outing started fast Spurs , sharp touch finish bottom corner in-form Salah giving lead 11 minute . Ivan Perisic header deflected onto post Liverpool goalkeeper Alisson Ryan Sessegnon saw penalty shout challenge Trent Alexander-Arnold waved away Spurs came life - Reds struck break thanks gift Eric Dier . The centre-back miscued header towards goal Salah ( 40 ) raced chip ninth goal eight game . Tottenham sent early second half - first time season - better break , Alisson pushing Perisic effort onto woodwork Harry Kane ( 70 ) fired home brilliant strike played sub Dejan Kulusevski . Rodrigo Bentancur went close couple header Kane glanced wide Spurs desperately sought another late goal Liverpool clung first away win Premier League season move eighth within seven point fourth-placed Spurs , game hand .', 'There growing popularity Hospital Incident Command System ( HICS ) organizational tool hospital management COVID-19 pandemic . We specifically describe implementation HICS Isfahan province reference hospital ( Isabn-e-Maryam ) COVID-19 pandemic try explore performance . Methods : To document action taken COVID-19 pandemic , standard , open-ended interview conducted individual occupying activated HICS leadership position event . A checklist based job action sheet HICS used performance assessment . Results : With onset pandemic , hospital director revised ICS structure adheres span better control COVID-19 . Methods expanding hospital inpatient capacity enable surge capacity considered . The highest performance score field planning . Performance intermediate Financial/Administration section good field . Discussion : In current COVID-19 pandemic , establishing HICS consideration long-standing event help improve communication , resource use , staff patient protection , maintenance role .', 'According Fidelity ’ Financial Resolutions survey , saving money number one-resolution respondent . Close half ( 43 % ) said goal wanted work toward new year . Building nest egg help pay big ticket item — like house , vacation , wedding even expensive item really want — without taking additional debt . Having saving also come handy emergency expense come . If ’ looking increase saving new year , help start small — even ’ transferring $ 10 week saving account . Starting small help build muscle saving . This way , receive salary bump , bonus gift money , ’ already gotten habit saving , ’ likely transfer money saving account . You may also want consider automating process instead manually moving money saving . Relying manual transfer leaf lot room procrastination — know , ’ spent money intended save . But set automatic transfer saving account , take away need make decision altogether . You usually schedule automatic transfer bank ’ mobile app . Lastly , want see saving grow little faster , opt high-yield saving account instead traditional saving account . High-yield saving account — like Marcus Goldman Sachs Online Savings Account Ally Online Savings Account — pay interest month compared traditional saving account .']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A new text to be classified based on topic\n",
        "query_text = \"Federal revenue for the period from January to September 2022 totalled approximately €256.7bn, up by 10.1% (about €23.6bn) on the year. Tax receipts (including EU own resources that are subtracted from the total) increased by 10.1% (about €22.0bn) on the year. Revenue from value added taxes rose by 22.2% (about €18.5bn), while receipts from income tax and corporation tax grew by 7.9% (about €8.9bn). Federal revenue fell as a result of a year-on-year increase of approximately €4.1bn in public transport subsidies to the Länder. These additional subsidies were used to offset revenue losses in the public transport sector and to finance the 9-euro ticket scheme (a temporary reduced-rate public transport ticket costing €9 per month in the months of June, July and August 2022).\"\n",
        "query_words = word_tokenize(query_text)\n",
        "query_words_clean = [lemmatizer.lemmatize(word) for word in query_words if word not in set(stopwords.words('english'))]\n",
        "query_words_corpus = [word for word in query_words_clean if word in set(bow_keys)]\n",
        "query_text_corpus = ' '.join(query_words_corpus)\n",
        "corpus_texts.append(query_text_corpus)\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer() ## Creating Object for CountVectorizer\n",
        "bow_vectors = cv.fit_transform(corpus_texts).toarray()\n",
        "print(bow_vectors)\n",
        "print(len(bow_vectors[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_FjnP-PkM-g",
        "outputId": "0566af67-39e8-4bfb-f3dd-7f2bbf20e662"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 1 0 ... 0 0 0]\n",
            " [0 0 5 ... 0 0 0]\n",
            " [1 0 0 ... 2 2 2]\n",
            " [0 0 0 ... 2 0 0]]\n",
            "330\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classification based on maximum similarity"
      ],
      "metadata": {
        "id": "dx6PW5u93IlM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize the BoW vectors\n",
        "bow_texts_norm = []\n",
        "for bow in bow_vectors:\n",
        "  length = (sum(i*i for i in bow)) ** 0.5\n",
        "  bow_norm = bow / length\n",
        "  bow_texts_norm.append(bow_norm)\n",
        "\n",
        "# Compute similarity using dot product\n",
        "similarity_vector = []\n",
        "bow_norm_query = bow_texts_norm[3]\n",
        "for bow in bow_texts_norm[:3]:\n",
        "  similarity_vector.append(sum(i*j for i,j in zip(bow,bow_norm_query)))\n",
        "print(similarity_vector)\n",
        "\n",
        "# Find the highest similarity\n",
        "id_max_sim = similarity_vector.index(max(similarity_vector))\n",
        "if (id_max_sim == 0):\n",
        "  print (\"The query text is classified as: Sport\")\n",
        "elif (id_max_sim == 1):\n",
        "  print (\"The query text is classified as: Medical\")\n",
        "elif (id_max_sim == 2):\n",
        "  print (\"The query text is classified as: Finance\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bv3Zx7M3hFuK",
        "outputId": "733cfb55-27c3-4c8d-e3f3-8418a24b03cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.0, 0.03214121732666125, 0.11158046066936295]\n",
            "The query text is classified as: Finance\n"
          ]
        }
      ]
    }
  ]
}